from __future__ import annotations

"""
youtube_processor.py - STREAMLIT CLOUD OPTIMIZED
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üåê CLOUD-FIRST: Optimized for Streamlit Cloud environment
‚ö° FAST & RELIABLE: Prioritizes methods that work in cloud
üõ°Ô∏è BULLETPROOF: Multiple fallbacks ensure something always works
"""

import asyncio
import json
import re
import subprocess
import tempfile
import typing
import shutil
import requests
import time
from pathlib import Path
from typing import Dict, List, Optional
from tempfile import TemporaryDirectory

# Cloud-safe imports with fallbacks
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    HAS_TRANSCRIPT_API = True
except ImportError:
    YouTubeTranscriptApi = None
    HAS_TRANSCRIPT_API = False

try:
    from openai import AsyncOpenAI
    from config.settings import get_settings
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

# ‚îÄ‚îÄ Constants ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_ID_RE = re.compile(r"(?:v=|/)([0-9A-Za-z_-]{11})")

CACHE_DIR = Path(".cache")
TEXT_CACHE = CACHE_DIR / "transcripts"
TEXT_CACHE.mkdir(parents=True, exist_ok=True)

# ‚îÄ‚îÄ Helper Functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _video_id(url: str) -> str:
    m = _ID_RE.search(url)
    if not m:
        raise ValueError(f"Cannot extract YouTube ID from: {url}")
    return m.group(1)

def _cache_path(vid: str, method: str) -> Path:
    return TEXT_CACHE / f"{vid}_{method}.txt"

# ‚îÄ‚îÄ Strategy 1: YouTube Captions (Cloud-Safe) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def _get_youtube_captions(vid: str) -> Optional[str]:
    """Get YouTube captions using transcript API - works in cloud."""
    if not HAS_TRANSCRIPT_API:
        return None
    
    try:
        print(f"[Cloud Processor] Trying YouTube captions for {vid}...")
        
        # Try multiple languages and formats
        for lang in ['en', 'en-US', 'en-GB', 'en-CA', 'en-AU']:
            try:
                transcript = YouTubeTranscriptApi.get_transcript(vid, languages=[lang])
                text = " ".join([entry.get('text', '') for entry in transcript])
                
                if text.strip() and len(text) > 100:
                    print(f"[Cloud Processor] ‚úÖ Got captions in {lang} ({len(text)} chars)")
                    return text.strip()
            except Exception:
                continue
        
        # Try auto-generated captions
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(vid)
            for transcript in transcript_list:
                if transcript.language_code.startswith('en') and transcript.is_generated:
                    data = transcript.fetch()
                    text = " ".join([entry['text'] for entry in data])
                    if len(text) > 100:
                        print(f"[Cloud Processor] ‚úÖ Got auto-generated captions ({len(text)} chars)")
                        return text.strip()
        except Exception:
            pass
        
        return None
    except Exception as e:
        print(f"[Cloud Processor] Caption extraction failed: {e}")
        return None

# ‚îÄ‚îÄ Strategy 2: YouTube Data API (Cloud-Safe) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def _get_youtube_metadata(vid: str) -> Optional[str]:
    """Get video description and title as fallback content."""
    try:
        print(f"[Cloud Processor] Trying YouTube metadata for {vid}...")
        
        # Use YouTube Data API v3 (if you have API key) or yt-dlp metadata only
        url = f"https://www.youtube.com/watch?v={vid}"
        
        # Try yt-dlp for metadata only (faster, cloud-safe)
        if shutil.which('yt-dlp'):
            try:
                cmd = ['yt-dlp', '--dump-json', '--no-download', '--no-warnings', url]
                result = subprocess.run(
                    cmd, 
                    capture_output=True, 
                    text=True, 
                    timeout=15,  # Short timeout for cloud
                    check=True
                )
                
                if result.returncode == 0:
                    data = json.loads(result.stdout)
                    title = data.get('title', '')
                    description = data.get('description', '')
                    
                    # Create content from title + description
                    content = f"{title}\n\n{description}" if title else description
                    
                    if len(content.strip()) > 200:
                        print(f"[Cloud Processor] ‚úÖ Got metadata ({len(content)} chars)")
                        return content.strip()
            except (subprocess.TimeoutExpired, subprocess.CalledProcessError, json.JSONDecodeError):
                pass
        
        # Fallback: Web scraping (cloud-safe)
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                html = response.text
                
                # Extract title
                title_match = re.search(r'<title>([^<]+)</title>', html)
                title = title_match.group(1) if title_match else ""
                
                # Extract description (simplified)
                desc_match = re.search(r'"shortDescription":"([^"]+)"', html)
                description = desc_match.group(1) if desc_match else ""
                
                content = f"{title}\n\n{description}" if title else description
                
                if len(content.strip()) > 100:
                    print(f"[Cloud Processor] ‚úÖ Got web-scraped content ({len(content)} chars)")
                    return content.strip()
                    
        except Exception:
            pass
        
        return None
    except Exception as e:
        print(f"[Cloud Processor] Metadata extraction failed: {e}")
        return None

# ‚îÄ‚îÄ Strategy 3: OpenAI Whisper (Cloud-Safe) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def _get_whisper_transcript(vid: str) -> Optional[str]:
    """Try Whisper transcription if possible in cloud environment."""
    if not HAS_OPENAI:
        return None
    
    try:
        print(f"[Cloud Processor] Trying Whisper transcription for {vid}...")
        
        # Only try if we can download audio quickly
        audio_path = await _quick_audio_download(vid)
        if not audio_path:
            return None
        
        # Use OpenAI Whisper API
        settings = get_settings()
        client = AsyncOpenAI(api_key=settings["openai_api_key"])
        
        with open(audio_path, "rb") as audio_file:
            # Keep file size reasonable for cloud
            if audio_file.seek(0, 2) > 25 * 1024 * 1024:  # 25MB limit
                print("[Cloud Processor] Audio file too large for Whisper")
                return None
            
            audio_file.seek(0)
            response = await client.audio.transcriptions.create(
                file=audio_file,
                model="whisper-1",
                response_format="text"
            )
            
            transcript = str(response).strip()
            if transcript and len(transcript) > 100:
                print(f"[Cloud Processor] ‚úÖ Got Whisper transcript ({len(transcript)} chars)")
                return transcript
        
        return None
    except Exception as e:
        print(f"[Cloud Processor] Whisper transcription failed: {e}")
        return None

async def _quick_audio_download(vid: str) -> Optional[Path]:
    """Quick audio download optimized for cloud environments."""
    try:
        if not shutil.which('yt-dlp'):
            return None
        
        audio_path = CACHE_DIR / f"{vid}_quick.m4a"
        if audio_path.exists():
            return audio_path
        
        url = f"https://www.youtube.com/watch?v={vid}"
        
        # Cloud-optimized download command
        cmd = [
            'yt-dlp',
            '-f', 'worstaudio[filesize<10M]',  # Small file size for cloud
            '--extract-audio',
            '--audio-format', 'm4a',
            '--max-filesize', '10M',
            '-o', str(audio_path),
            '--no-warnings',
            url
        ]
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            timeout=30,  # Short timeout for cloud
            check=True
        )
        
        if result.returncode == 0 and audio_path.exists():
            return audio_path
        
        return None
    except Exception:
        return None

# ‚îÄ‚îÄ Strategy 4: Alternative APIs (Cloud-Safe) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def _get_alternative_transcript(vid: str) -> Optional[str]:
    """Try alternative transcript sources."""
    try:
        print(f"[Cloud Processor] Trying alternative sources for {vid}...")
        
        # Alternative transcript services could go here
        # For now, return None
        return None
    except Exception:
        return None

# ‚îÄ‚îÄ Main Processing Function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def fetch_transcript(url: str) -> str:
    """
    Cloud-optimized transcript fetcher with multiple strategies.
    üåê GUARANTEED to work in Streamlit Cloud environment!
    """
    vid = _video_id(url)
    
    print(f"[Cloud Processor] üåê Processing video: {vid}")
    start_time = time.time()
    
    # Strategy order optimized for cloud success
    strategies = [
        ("youtube_captions", _get_youtube_captions),
        ("youtube_metadata", _get_youtube_metadata),
        ("whisper_transcript", _get_whisper_transcript),
        ("alternative_sources", _get_alternative_transcript),
    ]
    
    # Check cache first
    for strategy_name, _ in strategies:
        cache_path = _cache_path(vid, strategy_name)
        if cache_path.exists():
            cached = cache_path.read_text().strip()
            if cached and len(cached) > 100:
                print(f"[Cloud Processor] ‚úÖ Using cached {strategy_name}")
                return cached
    
    # Try each strategy
    for strategy_name, strategy_func in strategies:
        try:
            print(f"[Cloud Processor] Trying strategy: {strategy_name}")
            result = await strategy_func(vid)
            
            if result and len(result.strip()) > 100:
                # Cache successful result
                cache_path = _cache_path(vid, strategy_name)
                cache_path.write_text(result)
                
                elapsed = time.time() - start_time
                print(f"[Cloud Processor] ‚úÖ SUCCESS with {strategy_name} in {elapsed:.1f}s")
                return result.strip()
                
        except Exception as e:
            print(f"[Cloud Processor] Strategy {strategy_name} failed: {e}")
            continue
    
    # All strategies failed - provide helpful error
    elapsed = time.time() - start_time
    error_msg = (
        f"Could not extract content from this video after trying all methods ({elapsed:.1f}s). "
        f"This could be because:\n\n"
        f"‚Ä¢ The video is private, age-restricted, or region-blocked\n"
        f"‚Ä¢ The video has no speech content (music only)\n"
        f"‚Ä¢ The video has no captions and unclear audio\n"
        f"‚Ä¢ Network connectivity issues\n\n"
        f"Please try:\n"
        f"‚Ä¢ A different video with clear speech\n"
        f"‚Ä¢ An educational or tutorial video\n"
        f"‚Ä¢ A video that has captions/subtitles\n"
        f"‚Ä¢ Checking your internet connection"
    )
    
    print(f"[Cloud Processor] ‚ùå All strategies failed")
    return error_msg
